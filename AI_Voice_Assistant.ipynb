{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfGlmL7iioTa3Z5tOPwbw2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/work4ai/AI_Voice_Assistant/blob/main/AI_Voice_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAHI7XuUQdhW",
        "outputId": "5456a043-6aca-4e88-886b-e899451e0c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers gradio gtts torchvision torchaudio\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import whisper\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# Load Whisper\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Load BLIP\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "blip_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SvosJ2EQ1dN",
        "outputId": "7b5afd02-b94a-4360-d080-9466e9d8d618"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_path):\n",
        "    if audio_path is None:\n",
        "        return \"\"\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
        "    options = whisper.DecodingOptions(fp16=False)\n",
        "    result = whisper.decode(whisper_model, mel, options)\n",
        "    return result.text.strip()\n",
        "\n",
        "def analyze_image_vqa(image_path, question):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    prompt = question if question.strip() else \"Describe the image in detail.\"\n",
        "\n",
        "    inputs = blip_processor(image, prompt, return_tensors=\"pt\")\n",
        "    out = blip_model.generate(**inputs)\n",
        "    result = blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "    return result\n",
        "\n",
        "def speak_text(text, filename=\"response.mp3\"):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(filename)\n",
        "    return filename"
      ],
      "metadata": {
        "id": "rewg-AfAQ10F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(audio, image):\n",
        "    question = transcribe_audio(audio)\n",
        "    if not question:\n",
        "        question = \"Describe this image in detail.\"\n",
        "\n",
        "    answer = analyze_image_vqa(image, question)\n",
        "    audio_file = speak_text(answer)\n",
        "    return question, answer, audio_file\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Ask a Question\"),\n",
        "        gr.Image(type=\"filepath\", label=\"Upload Image\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Transcribed Question\"),\n",
        "        gr.Textbox(label=\"Image Description\"),\n",
        "        gr.Audio(label=\"Spoken Answer\")\n",
        "    ],\n",
        "    title=\"üñºÔ∏è Voice-Powered Image Description\",\n",
        "    description=\"Speak a question and upload an image. The app transcribes your voice, analyzes the image, and reads the answer aloud.\",\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "glWWFkYIQ2W-",
        "outputId": "044c60be-8b45-42ce-f2ad-d52fc37bb254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://dd387a95e8c80feb53.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dd387a95e8c80feb53.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}